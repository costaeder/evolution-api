# Estrat√©gia de Recupera√ß√£o de Erros e Suic√≠dio Controlado

**Data:** 2025-10-02
**Objetivo:** Implementar health checks e suic√≠dio controlado para erros fatais que deixam a Evolution API travada.

---

## 1. An√°lise dos Erros em `docs/errors.txt`

### üî¥ Erro 1: Connection Closed (FATAL - N√ÉO COBERTO)

**Ocorr√™ncia:** Linha 1-25 de `docs/errors.txt`

```
Error: Connection Closed
at sendRawMessage (/evolution/node_modules/baileys/lib/Socket/socket.js:60:19)
...
statusCode: 428,
payload: {
  statusCode: 428,
  error: 'Precondition Required',
  message: 'Connection Closed'
}
```

#### An√°lise:
- **Causa:** Conex√£o WebSocket com WhatsApp foi fechada
- **Momento:** Durante envio de mensagem via Chatwoot
- **Impacto:** ‚ö†Ô∏è **CR√çTICO** - A inst√¢ncia fica inutiliz√°vel
- **Estado atual:** ‚ùå **N√ÉO COBERTO** - Apenas logado, n√£o h√° recovery

#### Cobertura atual:
```typescript
// src/config/error.config.ts (atual)
process.on('uncaughtException', (error, origin) => {
  logger.error({ origin, stderr: process.stderr.fd, error });
  // ‚ùå N√£o faz NADA para se recuperar!
});
```

#### Problema:
Ap√≥s este erro, a API continua rodando mas:
- ‚úÖ Health check HTTP: Passa (servidor responde)
- ‚ùå Funcionalidade real: QUEBRADA (WhatsApp desconectado)
- ‚ùå Docker Swarm: N√ÉO detecta (container est√° "healthy")

---

### üî¥ Erro 2: uncaughtException - WebSocket Closed (FATAL - PARCIALMENTE COBERTO)

**Ocorr√™ncia:** Linha 28-44 de `docs/errors.txt`

```
{
  origin: 'uncaughtException',
  error: Error: WebSocket was closed before the connection was established
    at WebSocket.close (/evolution/node_modules/ws/lib/websocket.js:299:7)
    at WebSocketClient.close (/evolution/node_modules/baileys/lib/Socket/Client/websocket.js:53:21)
    at ts.restartInstance (/evolution/src/api/controllers/instance.controller.ts:345:30)
```

#### An√°lise:
- **Causa:** Tentativa de fechar WebSocket antes de conectar (race condition)
- **Momento:** Durante restart de inst√¢ncia
- **Impacto:** üî• **CR√çTICO** - Exception n√£o tratada, processo pode morrer ou ficar inconsistente
- **Estado atual:** ‚ö†Ô∏è **PARCIALMENTE COBERTO** - Logado mas n√£o h√° recovery

#### Cobertura atual:
```typescript
// src/config/error.config.ts (atual)
process.on('uncaughtException', (error, origin) => {
  logger.error({ origin, stderr: process.stderr.fd, error });
  // ‚ö†Ô∏è Apenas loga, processo continua em estado INCONSISTENTE
});
```

#### Problema:
- Node.js: Ap√≥s `uncaughtException`, o processo est√° em **estado indefinido**
- Best practice: Sempre fazer shutdown ap√≥s uncaughtException
- Realidade atual: **Processo continua rodando** (perigoso!)

---

### üü° Erro 3: Failed to Decrypt Message (N√ÉO-FATAL - ADEQUADAMENTE COBERTO)

**Ocorr√™ncia:** Linha 76-79 de `docs/errors.txt`

```
TypeError: Cannot create property 'senderMessageKeys' on number '91'
  at new SenderKeyState (/evolution/node_modules/baileys/WASignalGroup/sender_key_state.js:48:56)
  ...
msg=failed to decrypt message
```

#### An√°lise:
- **Causa:** Corrup√ß√£o de dados no sender key state (bug do Baileys)
- **Momento:** Recebimento de mensagens de status@broadcast
- **Impacto:** üü° **BAIXO** - Apenas mensagens de status n√£o s√£o decriptadas
- **Estado atual:** ‚úÖ **ADEQUADAMENTE COBERTO** - Erro capturado e logado pelo Baileys

#### Cobertura:
```typescript
// Baileys j√° trata internamente
// Loga o erro mas continua funcionando
// N√£o afeta o funcionamento geral da API
```

---

## 2. Resumo de Cobertura

| Erro | Gravidade | Cobertura Atual | Causa Travamento? | Precisa Suic√≠dio? |
|------|-----------|-----------------|-------------------|-------------------|
| Connection Closed | üî¥ CR√çTICA | ‚ùå N√£o coberto | ‚úÖ SIM | ‚úÖ SIM |
| uncaughtException (WebSocket) | üî• CR√çTICA | ‚ö†Ô∏è Parcial (apenas log) | ‚úÖ SIM | ‚úÖ SIM |
| Failed to decrypt | üü° BAIXA | ‚úÖ Coberto | ‚ùå N√£o | ‚ùå N√£o |

**Conclus√£o:** 2 de 3 erros precisam de implementa√ß√£o de **suic√≠dio controlado**.

---

## 3. Por que a API "Trava" sem Avisar

### Problema Atual

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Evolution API      ‚îÇ
‚îÇ  (container)        ‚îÇ
‚îÇ                     ‚îÇ
‚îÇ  HTTP Server: ‚úÖ    ‚îÇ  ‚Üê Health check passa
‚îÇ  WebSocket WA: ‚ùå   ‚îÇ  ‚Üê Conex√£o morreu
‚îÇ  Funcionalidade: ‚ùå ‚îÇ  ‚Üê N√£o envia/recebe mensagens
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üë
         ‚îÇ
    Docker Swarm diz: "Container healthy!" üò±
```

### Por que acontece:

1. **Erro fatal ocorre** (Connection Closed, uncaughtException)
2. **Processo Node.js:** Continua rodando (apenas logou)
3. **HTTP Server:** Continua respondendo (Express ainda funciona)
4. **Health check:** ‚úÖ Passa (porta 8080 responde)
5. **Docker Swarm:** üò¥ "T√° tudo bem!"
6. **Realidade:** üíÄ WhatsApp desconectado, mensagens n√£o chegam

**Resultado:** Container zumbi - vivo mas n√£o funcional.

---

## 4. Solu√ß√£o: Suic√≠dio Controlado

### Conceito

Quando um **erro fatal** √© detectado:

```
1. ‚ö†Ô∏è  Erro fatal detectado
2. üìù Logar detalhes completos
3. üì° Enviar alerta via webhook (opcional)
4. ‚è±Ô∏è  Aguardar 500ms para logs serem escritos
5. üíÄ process.exit(1)
6. üîÑ Docker Swarm detecta exit code 1
7. ‚úÖ Docker Swarm recria container SAUD√ÅVEL
```

### Vantagens

- ‚úÖ Container sempre funcional (ou morto)
- ‚úÖ Sem containers zumbi
- ‚úÖ Auto-recupera√ß√£o autom√°tica
- ‚úÖ Alertas via webhook
- ‚úÖ Logs completos antes de morrer

### Desvantagens

- ‚ö†Ô∏è Breve downtime (~5-10s durante restart)
- ‚ö†Ô∏è Conex√µes WebSocket ativas s√£o perdidas (mas j√° estavam quebradas)
- ‚ö†Ô∏è Se erro for persistente (ex: banco fora), vai ficar em loop (mas isso √© melhor que travar)

---

## 5. Implementa√ß√£o

### 5.1. Novo `src/config/error.config.ts`

Arquivo completo em: `src/config/error.config.ts` (ver c√≥digo anexo)

**Principais features:**

```typescript
// 1. Lista de erros fatais
const FATAL_ERROR_PATTERNS = [
  /Connection Closed/i,
  /WebSocket was closed before the connection/i,
  /ECONNREFUSED/i,
  /ETIMEDOUT/i,
  /socket hang up/i,
];

// 2. Contador de erros (evita suic√≠dio por erro transiente √∫nico)
let fatalErrorCount = 0;
const FATAL_ERROR_THRESHOLD = 3; // Ap√≥s 3 erros consecutivos

// 3. Graceful shutdown
async function gracefulShutdown(error, origin, logger) {
  logger.error('üíÄ FATAL ERROR - INITIATING CONTROLLED SUICIDE');

  // Logar tudo
  logger.error({ error, origin, pid, uptime, memoryUsage });

  // Enviar webhook de alerta
  await sendErrorAlert(error, origin);

  // Aguardar logs serem escritos
  await sleep(500);

  // Matar processo
  process.exit(1); // Docker Swarm recria
}

// 4. Handler de uncaughtException (SEMPRE FATAL)
process.on('uncaughtException', async (error, origin) => {
  // uncaughtException = processo em estado inconsistente
  // Melhor a√ß√£o: SEMPRE fazer shutdown
  await gracefulShutdown(error, origin, logger);
});

// 5. Handler de unhandledRejection (CONDICIONALMENTE FATAL)
process.on('unhandledRejection', async (reason, promise) => {
  const error = reason instanceof Error ? reason : new Error(reason);

  // Verificar se √© erro fatal
  if (isFatalError(error)) {
    fatalErrorCount++;

    if (fatalErrorCount >= FATAL_ERROR_THRESHOLD) {
      // 3 erros fatais consecutivos = suic√≠dio
      await gracefulShutdown(error, 'unhandledRejection', logger);
    }
  }
});
```

---

### 5.2. Health Check HTTP Endpoint (Opcional mas Recomendado)

**Arquivo:** `src/api/controllers/health.controller.ts`

```typescript
import { WAMonitoringService } from '@api/services/monitor.service';

export class HealthController {
  constructor(private readonly waMonitor: WAMonitoringService) {}

  /**
   * Health check simples (j√° existe)
   * GET /health
   */
  public async health() {
    return {
      status: 'ok',
      timestamp: new Date().toISOString(),
      uptime: process.uptime(),
    };
  }

  /**
   * Health check PROFUNDO (novo)
   * GET /health/deep
   *
   * Verifica se WhatsApp est√° realmente conectado
   */
  public async deepHealth() {
    const instances = this.waMonitor.waInstances;
    const instanceKeys = Object.keys(instances);

    if (instanceKeys.length === 0) {
      return {
        status: 'degraded',
        reason: 'No instances configured',
        timestamp: new Date().toISOString(),
      };
    }

    // Verificar se pelo menos uma inst√¢ncia est√° conectada
    const connectedInstances = instanceKeys.filter(key => {
      try {
        const instance = instances[key];
        return instance?.client?.state === 'open';
      } catch {
        return false;
      }
    });

    if (connectedInstances.length === 0) {
      // NENHUMA inst√¢ncia conectada = UNHEALTHY
      return {
        status: 'unhealthy',
        reason: 'No WhatsApp instances connected',
        instances: {
          total: instanceKeys.length,
          connected: 0,
          disconnected: instanceKeys.length,
        },
        timestamp: new Date().toISOString(),
      };
    }

    return {
      status: 'healthy',
      instances: {
        total: instanceKeys.length,
        connected: connectedInstances.length,
        disconnected: instanceKeys.length - connectedInstances.length,
      },
      timestamp: new Date().toISOString(),
    };
  }
}
```

**Router:** `src/api/routes/health.router.ts`

```typescript
import { Router } from 'express';
import { HealthController } from '@api/controllers/health.controller';
import { waMonitor } from '@api/server.module';

const router = Router();
const healthController = new HealthController(waMonitor);

// Health check simples
router.get('/health', async (req, res) => {
  const result = await healthController.health();
  res.status(200).json(result);
});

// Health check profundo
router.get('/health/deep', async (req, res) => {
  const result = await healthController.deepHealth();

  const statusCode =
    result.status === 'healthy' ? 200 :
    result.status === 'degraded' ? 200 : // Ainda aceit√°vel
    503; // unhealthy = Service Unavailable

  res.status(statusCode).json(result);
});

export default router;
```

---

### 5.3. Docker Compose / Swarm Config

**Atualizar `docker-compose.yaml`:**

```yaml
services:
  evolution:
    image: evolution-api:latest

    # Health check usando endpoint profundo
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health/deep"]
      interval: 30s      # Verifica a cada 30s
      timeout: 10s       # Timeout de 10s
      retries: 3         # 3 falhas consecutivas = unhealthy
      start_period: 40s  # Aguarda 40s antes de come√ßar health checks

    # Restart policy
    restart: unless-stopped

    # Ou, se usar Docker Swarm:
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
```

**Comportamento:**

```
1. Container inicia
2. Aguarda 40s (start_period)
3. Come√ßa health checks a cada 30s
4. Se /health/deep retorna 503 (unhealthy):
   - Tenta 3 vezes (retries)
   - Se 3 falhas consecutivas: marca container como unhealthy
   - Docker Swarm MATA e RECRIA
5. Novo container saud√°vel assume
```

---

## 6. Estrat√©gia de Deploy

### Fase 1: Adicionar Suic√≠dio Controlado (Seguro)

```bash
# 1. Substituir src/config/error.config.ts
git checkout feature/error-recovery
cp docs/error-recovery-strategy/error.config.ts src/config/error.config.ts

# 2. Build
npm run build

# 3. Deploy em staging
# Container com erro fatal vai se matar e recriar

# 4. Monitorar logs
docker service logs -f evolution_api

# Buscar por:
# - "üíÄ FATAL ERROR DETECTED"
# - "üîÑ Docker Swarm will restart"
```

**Risco:** üü¢ BAIXO - Melhora estabilidade, n√£o quebra nada

---

### Fase 2: Adicionar Health Check Profundo (Recomendado)

```bash
# 1. Adicionar health check controller
cp docs/error-recovery-strategy/health.controller.ts src/api/controllers/
cp docs/error-recovery-strategy/health.router.ts src/api/routes/

# 2. Registrar rota no index.router.ts
# Adicionar: import healthRouter from './health.router';
#           app.use('/', healthRouter);

# 3. Build
npm run build

# 4. Testar endpoint
curl http://localhost:8080/health/deep

# Deve retornar:
# {
#   "status": "healthy",
#   "instances": { ... }
# }

# 5. Atualizar docker-compose.yaml com healthcheck
# 6. Deploy
```

**Risco:** üü° M√âDIO - Altera health check (testar bem em staging)

---

## 7. Monitoramento P√≥s-Deploy

### Logs a Observar

```bash
# Container sendo recriado (suic√≠dio controlado)
docker service logs evolution_api | grep "FATAL ERROR DETECTED"

# Health checks falhando
docker service ps evolution_api --format "table {{.ID}}\t{{.Name}}\t{{.CurrentState}}"

# Containers em loop de restart (problema persistente)
docker stats evolution_api
```

### M√©tricas

| M√©trica | Valor Esperado | A√ß√£o se Fora |
|---------|---------------|--------------|
| Restarts por hora | < 2 | Investigar causa raiz |
| Tempo entre restarts | > 5 min | Problema persistente (ex: banco fora) |
| Health check failures | < 1% | OK |
| Health check failures | > 10% | Problema de infraestrutura |

---

## 8. Casos Especiais

### Caso 1: Loop de Restart Infinito

**Sintoma:**
```
Container reinicia a cada 5 segundos
Logs mostram sempre o mesmo erro fatal
```

**Causa poss√≠vel:**
- Banco de dados fora
- Redis fora
- Configura√ß√£o inv√°lida
- Bug no c√≥digo de inicializa√ß√£o

**A√ß√£o:**
```bash
# 1. Parar service
docker service scale evolution_api=0

# 2. Verificar depend√™ncias
docker exec -it postgres_container pg_isready
docker exec -it redis_container redis-cli ping

# 3. Verificar logs de inicializa√ß√£o
docker service logs evolution_api | head -50

# 4. Corrigir problema
# 5. Religar service
docker service scale evolution_api=1
```

---

### Caso 2: Erro Transiente Causa Restart Desnecess√°rio

**Sintoma:**
```
Erro de rede transiente (timeout moment√¢neo)
Container se mata mas n√£o precisava
```

**Solu√ß√£o:**
O c√≥digo j√° tem prote√ß√£o:

```typescript
const FATAL_ERROR_THRESHOLD = 3; // S√≥ suicida ap√≥s 3 erros

// Erros espa√ßados n√£o atingem threshold
// S√≥ erros consecutivos causam suic√≠dio
```

Se ainda ocorrer, ajustar threshold:
```typescript
const FATAL_ERROR_THRESHOLD = 5; // Mais tolerante
```

---

## 9. Checklist de Implementa√ß√£o

### Obrigat√≥rio (Fase 1)
- [ ] Substituir `src/config/error.config.ts` com nova vers√£o
- [ ] Testar em staging
- [ ] Verificar logs de suic√≠dio controlado
- [ ] Monitorar restarts por 24h
- [ ] Deploy em produ√ß√£o

### Recomendado (Fase 2)
- [ ] Adicionar health check controller
- [ ] Adicionar health check router
- [ ] Atualizar docker-compose.yaml com healthcheck profundo
- [ ] Testar endpoint `/health/deep`
- [ ] Deploy em produ√ß√£o

### Opcional (Fase 3)
- [ ] Adicionar Prometheus metrics
- [ ] Dashboard Grafana para monitorar restarts
- [ ] Alertas Slack/Discord para erros fatais
- [ ] PagerDuty para containers em loop

---

## 10. C√≥digo Fonte

### 10.1. `src/config/error.config.ts` (NOVO)

Ver arquivo anexo: `docs/error-recovery-strategy/error.config.ts`

### 10.2. `src/api/controllers/health.controller.ts` (NOVO)

Ver arquivo anexo: `docs/error-recovery-strategy/health.controller.ts`

### 10.3. `src/api/routes/health.router.ts` (NOVO)

Ver arquivo anexo: `docs/error-recovery-strategy/health.router.ts`

---

## 11. Refer√™ncias

- [Node.js Error Handling Best Practices](https://nodejs.org/api/process.html#process_event_uncaughtexception)
- [Docker Health Checks](https://docs.docker.com/engine/reference/builder/#healthcheck)
- [Docker Swarm Restart Policies](https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#restart-policy)
- Evolution API Issues:
  - #1234 - Connection Closed (se existir)
  - #5678 - WebSocket race condition (se existir)

---

## 12. FAQ

### Q: O suic√≠dio controlado n√£o vai causar downtime?

**A:** Sim, mas:
- Downtime: ~5-10 segundos (restart do container)
- Alternativa atual: Container zumbi INFINITAMENTE (downtime 100%)
- **Melhor:** 10s de downtime que se recupera sozinho

### Q: E se o erro for persistente? N√£o vai ficar em loop?

**A:** Sim, mas isso √© **bom**:
- Loop de restart = VIS√çVEL (voc√™ v√™ nos logs/monitoring)
- Container zumbi = INVIS√çVEL (parece que est√° OK)
- Com loop, voc√™ SABE que tem problema e pode investigar

### Q: Preciso realmente do health check profundo?

**A:** Depende:
- **M√≠nimo:** Suic√≠dio controlado (Fase 1) - j√° resolve 90%
- **Ideal:** Suic√≠dio + Health check profundo - resolve 100% + detec√ß√£o proativa

### Q: O threshold de 3 erros n√£o √© muito baixo?

**A:** Pode ajustar:
```typescript
const FATAL_ERROR_THRESHOLD = 5; // Mais tolerante
const FATAL_ERROR_THRESHOLD = 1; // Mais agressivo (suicida no primeiro erro fatal)
```

Recomenda√ß√£o: Come√ßar com 3, monitorar por 1 semana, ajustar se necess√°rio.

---

## 13. Conclus√£o

### Estado Atual
- ‚ùå Containers zumbi ficam rodando
- ‚ùå Sem detec√ß√£o de problemas reais
- ‚ùå Sem auto-recupera√ß√£o

### Estado Ap√≥s Implementa√ß√£o
- ‚úÖ Erros fatais causam restart controlado
- ‚úÖ Container sempre funcional (ou morto + recriando)
- ‚úÖ Logs completos + alertas webhook
- ‚úÖ Health check profundo detecta problemas
- ‚úÖ Auto-recupera√ß√£o autom√°tica

### Pr√≥ximos Passos
1. Implementar Fase 1 (suic√≠dio controlado)
2. Monitorar por 1 semana
3. Implementar Fase 2 (health check profundo)
4. Adicionar monitoring/alertas (Fase 3 - opcional)
